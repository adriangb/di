{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"di : dependency injection toolkit di is a modern dependency injection toolkit, modeled around the simplicity of FastAPI's dependency injection. Key features: Intuitive : simple API, inspired by FastAPI . Auto-wiring : di supports auto-wiring using type annotations. Scopes : inspired by pytest scopes , but defined by users (no fixed \"request\" or \"session\" scopes). Composable : decoupled internal APIs give you the flexibility to customize wiring, execution and binding. Performant : di can execute dependencies in parallel, move sync dependencies to threads and cache results. Performance critical parts are written in \ud83e\udd80 via graphlib2 . Installation pip install di [ anyio ] \u26a0\ufe0f This project is a work in progress. Until there is 1.X.Y release, expect breaking changes. \u26a0\ufe0f Simple Example Here is a simple example of how di works: from dataclasses import dataclass from di import Container , Dependant , SyncExecutor class A : ... class B : ... @dataclass class C : a : A b : B def main (): container = Container () executor = SyncExecutor () solved = container . solve ( Dependant ( C , scope = \"request\" ), scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : c = container . execute_sync ( solved , executor = executor , state = state ) assert isinstance ( c , C ) assert isinstance ( c . a , A ) assert isinstance ( c . b , B ) For more examples, see our docs . Why do I need dependency injection in Python? Isn't that a Java thing? Dependency injection is a software architecture technique that helps us achieve inversion of control and dependency inversion (one of the five SOLID design principles). It is a common misconception that traditional software design principles do not apply to Python. As a matter of fact, you are probably using a lot of these techniques already! For example, the transport argument to httpx's Client ( docs ) is an excellent example of dependency injection. Pytest, arguably the most popular Python test framework, uses dependency injection in the form of pytest fixtures . Most web frameworks employ inversion of control: when you define a view / controller, the web framework calls you! The same thing applies to CLIs (like click ) or TUIs (like Textual ). This is especially true for many newer web frameworks that not only use inversion of control but also dependency injection. Two great examples of this are FastAPI and BlackSheep . For a more comprehensive overview of Python projects related to dependency injection, see Awesome Dependency Injection in Python . Project Aims This project aims to be a dependency injection toolkit, with a focus on providing the underlying dependency injection functionality for other libraries. In other words, while you could use this as a standalone dependency injection framework, you may find it to be a bit terse and verbose. There are also much more mature standalone dependency injection frameworks; I would recommend at least looking into python-dependency-injector since it is currently the most popular / widely used of the bunch. For more background, see our docs .","title":"Intro"},{"location":"#di-dependency-injection-toolkit","text":"di is a modern dependency injection toolkit, modeled around the simplicity of FastAPI's dependency injection. Key features: Intuitive : simple API, inspired by FastAPI . Auto-wiring : di supports auto-wiring using type annotations. Scopes : inspired by pytest scopes , but defined by users (no fixed \"request\" or \"session\" scopes). Composable : decoupled internal APIs give you the flexibility to customize wiring, execution and binding. Performant : di can execute dependencies in parallel, move sync dependencies to threads and cache results. Performance critical parts are written in \ud83e\udd80 via graphlib2 .","title":"di: dependency injection toolkit"},{"location":"#installation","text":"pip install di [ anyio ] \u26a0\ufe0f This project is a work in progress. Until there is 1.X.Y release, expect breaking changes. \u26a0\ufe0f","title":"Installation"},{"location":"#simple-example","text":"Here is a simple example of how di works: from dataclasses import dataclass from di import Container , Dependant , SyncExecutor class A : ... class B : ... @dataclass class C : a : A b : B def main (): container = Container () executor = SyncExecutor () solved = container . solve ( Dependant ( C , scope = \"request\" ), scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : c = container . execute_sync ( solved , executor = executor , state = state ) assert isinstance ( c , C ) assert isinstance ( c . a , A ) assert isinstance ( c . b , B ) For more examples, see our docs .","title":"Simple Example"},{"location":"#why-do-i-need-dependency-injection-in-python-isnt-that-a-java-thing","text":"Dependency injection is a software architecture technique that helps us achieve inversion of control and dependency inversion (one of the five SOLID design principles). It is a common misconception that traditional software design principles do not apply to Python. As a matter of fact, you are probably using a lot of these techniques already! For example, the transport argument to httpx's Client ( docs ) is an excellent example of dependency injection. Pytest, arguably the most popular Python test framework, uses dependency injection in the form of pytest fixtures . Most web frameworks employ inversion of control: when you define a view / controller, the web framework calls you! The same thing applies to CLIs (like click ) or TUIs (like Textual ). This is especially true for many newer web frameworks that not only use inversion of control but also dependency injection. Two great examples of this are FastAPI and BlackSheep . For a more comprehensive overview of Python projects related to dependency injection, see Awesome Dependency Injection in Python .","title":"Why do I need dependency injection in Python? Isn't that a Java thing?"},{"location":"#project-aims","text":"This project aims to be a dependency injection toolkit, with a focus on providing the underlying dependency injection functionality for other libraries. In other words, while you could use this as a standalone dependency injection framework, you may find it to be a bit terse and verbose. There are also much more mature standalone dependency injection frameworks; I would recommend at least looking into python-dependency-injector since it is currently the most popular / widely used of the bunch. For more background, see our docs .","title":"Project Aims"},{"location":"architecture/","text":"Architecture The fundamental design principle of di is to split up the complexity of dependency injection into smaller component parts: Wiring: when we discover the dependencies. This includes doing reflection (inspecting signatures), looking for dependency markers, etc. Solving: when we build an execution plan, taking into account binds. Execution: when we execute dependencies, possibly doing IO, parallelization, etc. We map these responsibilities to well-defined classes/interfaces: Wiring: this is handled by Dependant Solving: this is handled by Container Execution: this is handled by Executor s There are also some auxiliary support classes: SolvedDependant holds a reference of the result of solving (an executable DAG) that can then be executed at a later time. Fundamentally, our class diagram looks like this: Mermaid diagram source classDiagram SolvedDependant \"1..n\" --o Dependant: aggregates into a DAG Container --> Dependant: visits sub-dependencies Container --> Executor: delegates execution Container --> SolvedDependant: stores solved DAG Container --> SolvedDependant: executes solved DAG class Dependant{ +get_dependencies() list~Dependant~ +register_parameter() Dependant } class SolvedDependant{ +dag Mapping~Dependant, SetOfDependant~ } class Executor{ +execute() } class Container{ +bind() +enter_scope(Scope) Container +solve(Dependant) SolvedDependant +execute(SolvedDependant, Executor) Result }","title":"Architecture"},{"location":"architecture/#architecture","text":"The fundamental design principle of di is to split up the complexity of dependency injection into smaller component parts: Wiring: when we discover the dependencies. This includes doing reflection (inspecting signatures), looking for dependency markers, etc. Solving: when we build an execution plan, taking into account binds. Execution: when we execute dependencies, possibly doing IO, parallelization, etc. We map these responsibilities to well-defined classes/interfaces: Wiring: this is handled by Dependant Solving: this is handled by Container Execution: this is handled by Executor s There are also some auxiliary support classes: SolvedDependant holds a reference of the result of solving (an executable DAG) that can then be executed at a later time. Fundamentally, our class diagram looks like this: Mermaid diagram source classDiagram SolvedDependant \"1..n\" --o Dependant: aggregates into a DAG Container --> Dependant: visits sub-dependencies Container --> Executor: delegates execution Container --> SolvedDependant: stores solved DAG Container --> SolvedDependant: executes solved DAG class Dependant{ +get_dependencies() list~Dependant~ +register_parameter() Dependant } class SolvedDependant{ +dag Mapping~Dependant, SetOfDependant~ } class Executor{ +execute() } class Container{ +bind() +enter_scope(Scope) Container +solve(Dependant) SolvedDependant +execute(SolvedDependant, Executor) Result }","title":"Architecture"},{"location":"binds/","text":"Binds Provider binding serves two important functions: A way to tell the container how to assemble things that can't be auto-wired, for example interfaces. A way to override dependencies in tests. Every bind in di consists of: A target callable: this can be a function, an interface / protocol or a concrete class A substitute dependency: an object implementing the DependantBase , usually just an instance of Dependant This means that binds are themselves dependencies: import sys from dataclasses import dataclass if sys . version_info < ( 3 , 8 ): from typing_extensions import Protocol else : from typing import Protocol from di.container import Container , bind_by_type from di.dependant import Dependant from di.executors import AsyncExecutor class DBProtocol ( Protocol ): async def execute ( self , sql : str ) -> None : ... async def controller ( db : DBProtocol ) -> None : await db . execute ( \"SELECT *\" ) @dataclass class DBConfig : host : str = \"localhost\" class Postgres ( DBProtocol ): def __init__ ( self , config : DBConfig ) -> None : self . host = config . host async def execute ( self , sql : str ) -> None : print ( sql ) async def framework () -> None : container = Container () container . bind ( bind_by_type ( Dependant ( Postgres , scope = \"request\" ), DBProtocol )) solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ]) # this next line would fail without the bind async with container . enter_scope ( \"request\" ) as state : await container . execute_async ( solved , executor = AsyncExecutor (), state = state ) # and we can double check that the bind worked # by requesting the instance directly async with container . enter_scope ( \"request\" ) as state : db = await container . execute_async ( container . solve ( Dependant ( DBProtocol ), scopes = [ \"request\" ]), executor = AsyncExecutor (), state = state , ) assert isinstance ( db , Postgres ) In this example we register the Postgres class to DBProtocol , and we can see that di auto-wires Postgres as well! Binds can be used as a direct function call, in which case they are permanent, or as a context manager, in which case they are reversed when the context manager exits. Bind hooks Binding is implemented as hooks / callbacks: when we solve a dependency graph, every hook is called with every dependant and if the hook \"matches\" the dependent it returns the substitute dependency (otherwise it just returns None ). This means you can implement any sort of matching you want, including: Matching by type (see di.container.bind_by_type ) Matching by any subclass ( di.container.bind_by_type using the covariant=True parameter) Custom logic, in the form of a bind hook ( Container.bind ) For example, to match by parameter name: import inspect import typing from dataclasses import dataclass from di.api.dependencies import DependantBase from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor @dataclass class Foo : bar : str = \"bar\" def match_by_parameter_name ( param : typing . Optional [ inspect . Parameter ], dependant : DependantBase [ typing . Any ] ) -> typing . Optional [ DependantBase [ typing . Any ]]: if param is not None and param . name == \"bar\" : return Dependant ( lambda : \"baz\" , scope = None ) return None container = Container () container . bind ( match_by_parameter_name ) solved = container . solve ( Dependant ( Foo , scope = None ), scopes = [ None ]) def main (): with container . enter_scope ( None ) as state : foo = container . execute_sync ( solved , executor = SyncExecutor (), state = state ) assert foo . bar == \"baz\"","title":"Registration and Binding"},{"location":"binds/#binds","text":"Provider binding serves two important functions: A way to tell the container how to assemble things that can't be auto-wired, for example interfaces. A way to override dependencies in tests. Every bind in di consists of: A target callable: this can be a function, an interface / protocol or a concrete class A substitute dependency: an object implementing the DependantBase , usually just an instance of Dependant This means that binds are themselves dependencies: import sys from dataclasses import dataclass if sys . version_info < ( 3 , 8 ): from typing_extensions import Protocol else : from typing import Protocol from di.container import Container , bind_by_type from di.dependant import Dependant from di.executors import AsyncExecutor class DBProtocol ( Protocol ): async def execute ( self , sql : str ) -> None : ... async def controller ( db : DBProtocol ) -> None : await db . execute ( \"SELECT *\" ) @dataclass class DBConfig : host : str = \"localhost\" class Postgres ( DBProtocol ): def __init__ ( self , config : DBConfig ) -> None : self . host = config . host async def execute ( self , sql : str ) -> None : print ( sql ) async def framework () -> None : container = Container () container . bind ( bind_by_type ( Dependant ( Postgres , scope = \"request\" ), DBProtocol )) solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ]) # this next line would fail without the bind async with container . enter_scope ( \"request\" ) as state : await container . execute_async ( solved , executor = AsyncExecutor (), state = state ) # and we can double check that the bind worked # by requesting the instance directly async with container . enter_scope ( \"request\" ) as state : db = await container . execute_async ( container . solve ( Dependant ( DBProtocol ), scopes = [ \"request\" ]), executor = AsyncExecutor (), state = state , ) assert isinstance ( db , Postgres ) In this example we register the Postgres class to DBProtocol , and we can see that di auto-wires Postgres as well! Binds can be used as a direct function call, in which case they are permanent, or as a context manager, in which case they are reversed when the context manager exits.","title":"Binds"},{"location":"binds/#bind-hooks","text":"Binding is implemented as hooks / callbacks: when we solve a dependency graph, every hook is called with every dependant and if the hook \"matches\" the dependent it returns the substitute dependency (otherwise it just returns None ). This means you can implement any sort of matching you want, including: Matching by type (see di.container.bind_by_type ) Matching by any subclass ( di.container.bind_by_type using the covariant=True parameter) Custom logic, in the form of a bind hook ( Container.bind ) For example, to match by parameter name: import inspect import typing from dataclasses import dataclass from di.api.dependencies import DependantBase from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor @dataclass class Foo : bar : str = \"bar\" def match_by_parameter_name ( param : typing . Optional [ inspect . Parameter ], dependant : DependantBase [ typing . Any ] ) -> typing . Optional [ DependantBase [ typing . Any ]]: if param is not None and param . name == \"bar\" : return Dependant ( lambda : \"baz\" , scope = None ) return None container = Container () container . bind ( match_by_parameter_name ) solved = container . solve ( Dependant ( Foo , scope = None ), scopes = [ None ]) def main (): with container . enter_scope ( None ) as state : foo = container . execute_sync ( solved , executor = SyncExecutor (), state = state ) assert foo . bar == \"baz\"","title":"Bind hooks"},{"location":"cache/","text":"Dependency Cache Often, you will have dependencies that share a sub dependency. For example, you probably only want to load your configuration from environment variables once and then re-use the same object in multiple dependencies. To avoid re-computing the shared dependency, di will cache shared dependencies. How caching works Dependencies are cached by their cache key, computed in Dependant.cache_key . See dependants for more information on Dependant.cache_key . Dependencies are cached by default, but this behavior can be changed on a per-dependency basis using the use_cache=False parameter to Dependant . from random import random from di.container import Container from di.dependant import Dependant , Marker from di.executors import SyncExecutor from di.typing import Annotated def controller ( # no marker is equivalent to Dependant(object) v1 : object , # the default value is use_cache=True v2 : Annotated [ object , Marker ( object , scope = \"request\" )], # but you can set use_cache=False v3 : Annotated [ float , Marker ( random , use_cache = False , scope = \"request\" )], ) -> None : assert v1 is v2 assert v1 is not v3 and v2 is not v3 def main () -> None : container = Container () solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : container . execute_sync ( solved , executor = SyncExecutor (), state = state ) Caching and scopes Dependencies are cached within their scope and any inner scopes. Once a dependency's scope exits, it's cached value is discarded and the next time the scope is entered a fresh value will be computed.","title":"Caching"},{"location":"cache/#dependency-cache","text":"Often, you will have dependencies that share a sub dependency. For example, you probably only want to load your configuration from environment variables once and then re-use the same object in multiple dependencies. To avoid re-computing the shared dependency, di will cache shared dependencies.","title":"Dependency Cache"},{"location":"cache/#how-caching-works","text":"Dependencies are cached by their cache key, computed in Dependant.cache_key . See dependants for more information on Dependant.cache_key . Dependencies are cached by default, but this behavior can be changed on a per-dependency basis using the use_cache=False parameter to Dependant . from random import random from di.container import Container from di.dependant import Dependant , Marker from di.executors import SyncExecutor from di.typing import Annotated def controller ( # no marker is equivalent to Dependant(object) v1 : object , # the default value is use_cache=True v2 : Annotated [ object , Marker ( object , scope = \"request\" )], # but you can set use_cache=False v3 : Annotated [ float , Marker ( random , use_cache = False , scope = \"request\" )], ) -> None : assert v1 is v2 assert v1 is not v3 and v2 is not v3 def main () -> None : container = Container () solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : container . execute_sync ( solved , executor = SyncExecutor (), state = state )","title":"How caching works"},{"location":"cache/#caching-and-scopes","text":"Dependencies are cached within their scope and any inner scopes. Once a dependency's scope exits, it's cached value is discarded and the next time the scope is entered a fresh value will be computed.","title":"Caching and scopes"},{"location":"contributing/","text":"Developer setup This is a pure Python project and should be straightforward to set up on Linux or MacOS. We do not support Windows for development, if you use Windows you'll have to use VSCode DevContainers or a similar solution. We use Poetry for dependency management, and most of the config is the pyproject.toml . Linting is done via git hooks, managed by pre-commit . The linters may change over time, but they are configured in our pre-commit-config.yaml . Most of the setup and interaction with these systems is encapsulated in our Makefile . Project setup First, fork the repo and then clone your fork: $ git clone https://github.com/adriangb/di.git ---> 100% $ cd di Now install the project dependencies. You will need Make installed along with a compatible Python version (currently, 3.9.X). To set up the project, simply run: $ make init This will create a .venv virtualenv that you can configure your IDE to use. Running tests $ make test Tests are run with pytest, so you can also run them manually or configure your IDE to run them. The tests are stored in the tests/ directory. Running linting Linting will run automatically on every commit. To disable this, you can commit with git commit --no-verify . You can also run linting manually: $ make lint Documentation The docs are written as markdown and built with MkDocs. Both the docs and their source code are stored in the docs/ directory. To preview the docs locally as you edit them, run $ make docs-serve All the code fragments in the docs are stored as .py files in docs/src . These code fragments are tested as part of unit tests to ensure that the documentation stays up to date with the API. Releases This project uses continuous integration and continuous delivery on a trunk based workflow. Every merge into main should be fully functional code in a releasable state. As part of your pull request, you should propose what type of change is being made and determine the right version bump appropriately. While conventional commits are appreciated as a means of communication, especially for the merge commit, they are not required or enforced. You are however required to bump the package version in pyproject.toml . Every commit into main needs a version bump so that a release can be made, even if it is a refactor or \"chore\" type change. Once your change is merged, the new docs and PyPi package will be released automatically. Every time a release is made on PyPi, a corresponding GitHub release will be created to correlate PyPi versions to git commits.","title":"Contributing"},{"location":"contributing/#developer-setup","text":"This is a pure Python project and should be straightforward to set up on Linux or MacOS. We do not support Windows for development, if you use Windows you'll have to use VSCode DevContainers or a similar solution. We use Poetry for dependency management, and most of the config is the pyproject.toml . Linting is done via git hooks, managed by pre-commit . The linters may change over time, but they are configured in our pre-commit-config.yaml . Most of the setup and interaction with these systems is encapsulated in our Makefile .","title":"Developer setup"},{"location":"contributing/#project-setup","text":"First, fork the repo and then clone your fork: $ git clone https://github.com/adriangb/di.git ---> 100% $ cd di Now install the project dependencies. You will need Make installed along with a compatible Python version (currently, 3.9.X). To set up the project, simply run: $ make init This will create a .venv virtualenv that you can configure your IDE to use.","title":"Project setup"},{"location":"contributing/#running-tests","text":"$ make test Tests are run with pytest, so you can also run them manually or configure your IDE to run them. The tests are stored in the tests/ directory.","title":"Running tests"},{"location":"contributing/#running-linting","text":"Linting will run automatically on every commit. To disable this, you can commit with git commit --no-verify . You can also run linting manually: $ make lint","title":"Running linting"},{"location":"contributing/#documentation","text":"The docs are written as markdown and built with MkDocs. Both the docs and their source code are stored in the docs/ directory. To preview the docs locally as you edit them, run $ make docs-serve All the code fragments in the docs are stored as .py files in docs/src . These code fragments are tested as part of unit tests to ensure that the documentation stays up to date with the API.","title":"Documentation"},{"location":"contributing/#releases","text":"This project uses continuous integration and continuous delivery on a trunk based workflow. Every merge into main should be fully functional code in a releasable state. As part of your pull request, you should propose what type of change is being made and determine the right version bump appropriately. While conventional commits are appreciated as a means of communication, especially for the merge commit, they are not required or enforced. You are however required to bump the package version in pyproject.toml . Every commit into main needs a version bump so that a release can be made, even if it is a refactor or \"chore\" type change. Once your change is merged, the new docs and PyPi package will be released automatically. Every time a release is made on PyPi, a corresponding GitHub release will be created to correlate PyPi versions to git commits.","title":"Releases"},{"location":"dependants/","text":"Dependants and the DependantBase Most of these docs use Dependant as the main marker for dependencies. But the container doesn't actually know about either of these two things! In fact, the container only knows about the DependantBase , which you can find in di.api.dependencies . Dependant is just one possible implementation of the DependantBase . You can easily build your own version of Dependant by inheriting from Dependant or DependantBase . Here is an example that extracts headers from requests: import inspect from typing import Mapping , Optional , TypeVar from di.container import Container , bind_by_type from di.dependant import Dependant , Marker from di.executors import AsyncExecutor from di.typing import Annotated class Request : def __init__ ( self , headers : Mapping [ str , str ]) -> None : self . headers = { k . lower (): v for k , v in headers . items ()} class Header ( Marker ): def __init__ ( self , alias : Optional [ str ]) -> None : self . alias = alias super () . __init__ ( call = None , scope = \"request\" , use_cache = False ) def register_parameter ( self , param : inspect . Parameter ) -> Dependant [ str ]: if self . alias is not None : name = self . alias else : name = param . name . replace ( \"_\" , \"-\" ) def get_header ( request : Annotated [ Request , Marker ()]) -> str : return param . annotation ( request . headers [ name ]) return Dependant ( get_header , scope = \"request\" ) T = TypeVar ( \"T\" ) FromHeader = Annotated [ T , Header ( alias = None )] async def web_framework () -> None : container = Container () valid_request = Request ( headers = { \"x-header-one\" : \"one\" , \"x-header-two\" : \"2\" }) with container . bind ( bind_by_type ( Dependant ( lambda : valid_request , scope = \"request\" ), Request ) ): solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ] ) with container . enter_scope ( \"request\" ) as state : await container . execute_async ( solved , executor = AsyncExecutor (), state = state ) # success invalid_request = Request ( headers = { \"x-header-one\" : \"one\" }) with container . bind ( bind_by_type ( Dependant ( lambda : invalid_request , scope = \"request\" ), Request ) ): solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ] ) with container . enter_scope ( \"request\" ) as state : try : await container . execute_async ( solved , executor = AsyncExecutor (), state = state ) # fails except KeyError : pass else : raise AssertionError ( \"This call should have failed because x-header-two is missing\" ) def controller ( x_header_one : FromHeader [ str ], header_two_val : Annotated [ int , Header ( alias = \"x-header-two\" )], ) -> None : \"\"\"This is the only piece of user code\"\"\" assert x_header_one == \"one\" assert header_two_val == 2 Another good example of the flexibility provided by DependantBase is the implementation of JointDependant , which lets you schedule and execute dependencies together even if they are not directly connected by wiring: from di.container import Container from di.dependant import Dependant , JoinedDependant from di.executors import SyncExecutor class A : ... class B : executed = False def __init__ ( self ) -> None : B . executed = True def main (): container = Container () dependant = JoinedDependant ( Dependant ( A , scope = \"request\" ), siblings = [ Dependant ( B , scope = \"request\" )], ) solved = container . solve ( dependant , scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : a = container . execute_sync ( solved , executor = SyncExecutor (), state = state ) assert isinstance ( a , A ) assert B . executed Here B is executed even though A does not depend on it. This is because JoinedDependant leverages the DependantBase interface to tell di that B is a dependency of A even if B is not a parameter or otherwise related to A .","title":"Dependants"},{"location":"dependants/#dependants-and-the-dependantbase","text":"Most of these docs use Dependant as the main marker for dependencies. But the container doesn't actually know about either of these two things! In fact, the container only knows about the DependantBase , which you can find in di.api.dependencies . Dependant is just one possible implementation of the DependantBase . You can easily build your own version of Dependant by inheriting from Dependant or DependantBase . Here is an example that extracts headers from requests: import inspect from typing import Mapping , Optional , TypeVar from di.container import Container , bind_by_type from di.dependant import Dependant , Marker from di.executors import AsyncExecutor from di.typing import Annotated class Request : def __init__ ( self , headers : Mapping [ str , str ]) -> None : self . headers = { k . lower (): v for k , v in headers . items ()} class Header ( Marker ): def __init__ ( self , alias : Optional [ str ]) -> None : self . alias = alias super () . __init__ ( call = None , scope = \"request\" , use_cache = False ) def register_parameter ( self , param : inspect . Parameter ) -> Dependant [ str ]: if self . alias is not None : name = self . alias else : name = param . name . replace ( \"_\" , \"-\" ) def get_header ( request : Annotated [ Request , Marker ()]) -> str : return param . annotation ( request . headers [ name ]) return Dependant ( get_header , scope = \"request\" ) T = TypeVar ( \"T\" ) FromHeader = Annotated [ T , Header ( alias = None )] async def web_framework () -> None : container = Container () valid_request = Request ( headers = { \"x-header-one\" : \"one\" , \"x-header-two\" : \"2\" }) with container . bind ( bind_by_type ( Dependant ( lambda : valid_request , scope = \"request\" ), Request ) ): solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ] ) with container . enter_scope ( \"request\" ) as state : await container . execute_async ( solved , executor = AsyncExecutor (), state = state ) # success invalid_request = Request ( headers = { \"x-header-one\" : \"one\" }) with container . bind ( bind_by_type ( Dependant ( lambda : invalid_request , scope = \"request\" ), Request ) ): solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ] ) with container . enter_scope ( \"request\" ) as state : try : await container . execute_async ( solved , executor = AsyncExecutor (), state = state ) # fails except KeyError : pass else : raise AssertionError ( \"This call should have failed because x-header-two is missing\" ) def controller ( x_header_one : FromHeader [ str ], header_two_val : Annotated [ int , Header ( alias = \"x-header-two\" )], ) -> None : \"\"\"This is the only piece of user code\"\"\" assert x_header_one == \"one\" assert header_two_val == 2 Another good example of the flexibility provided by DependantBase is the implementation of JointDependant , which lets you schedule and execute dependencies together even if they are not directly connected by wiring: from di.container import Container from di.dependant import Dependant , JoinedDependant from di.executors import SyncExecutor class A : ... class B : executed = False def __init__ ( self ) -> None : B . executed = True def main (): container = Container () dependant = JoinedDependant ( Dependant ( A , scope = \"request\" ), siblings = [ Dependant ( B , scope = \"request\" )], ) solved = container . solve ( dependant , scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : a = container . execute_sync ( solved , executor = SyncExecutor (), state = state ) assert isinstance ( a , A ) assert B . executed Here B is executed even though A does not depend on it. This is because JoinedDependant leverages the DependantBase interface to tell di that B is a dependency of A even if B is not a parameter or otherwise related to A .","title":"Dependants and the DependantBase"},{"location":"examples/","text":"Examples Simple Example Here is a simple example of how di works: from dataclasses import dataclass from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor class A : ... class B : ... @dataclass class C : a : A b : B def main (): container = Container () solved = container . solve ( Dependant ( C , scope = \"request\" ), scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : c = container . execute_sync ( solved , executor = SyncExecutor (), state = state ) assert isinstance ( c , C ) assert isinstance ( c . a , A ) assert isinstance ( c . b , B ) You will notice that di \"auto-wired\" C : we didn't have to tell it that C depends on A and B , or how to construct A and B , it was all inferred from type annotations. In the wiring and provider registration chapters, you'll see how you can customize this behavior to tell di how to inject things like abstract interfaces or function return values. In-depth example With this background in place, let's dive into a more in-depth example. In this example, we'll look at what it would take for a web framework to provide dependency injection to its users via di . Let's start by looking at the User's code. from typing import Any , Callable from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value class App : def __init__ ( self , controller : Callable [ ... , Any ]) -> None : self . container = Container () self . solved = self . container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ], ) self . executor = SyncExecutor () def run ( self , request : Request ) -> int : with self . container . enter_scope ( \"request\" ) as state : return self . container . execute_sync ( self . solved , values = { Request : request }, executor = self . executor , state = state , ) # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) def main () -> None : app = App ( controller ) resp = app . run ( Request ( 1 )) assert resp == 2 resp = app . run ( Request ( 2 )) assert resp == 3 if __name__ == \"__main__\" : main () As a user, you have very little boilerplate. In fact, there is not a single line of code here that is not transmitting information. Now let's look at the web framework side of things. This part can get a bit complex, but it's okay because it's written once, in a library. First, we'll need to create a Container instance. This would be tied to the App or Router instance of the web framework. from typing import Any , Callable from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value class App : def __init__ ( self , controller : Callable [ ... , Any ]) -> None : self . container = Container () self . solved = self . container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ], ) self . executor = SyncExecutor () def run ( self , request : Request ) -> int : with self . container . enter_scope ( \"request\" ) as state : return self . container . execute_sync ( self . solved , values = { Request : request }, executor = self . executor , state = state , ) # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) def main () -> None : app = App ( controller ) resp = app . run ( Request ( 1 )) assert resp == 2 resp = app . run ( Request ( 2 )) assert resp == 3 if __name__ == \"__main__\" : main () Next we solve all of our endpoints/controllers (in this case just a single one). This should happen once, maybe at application startup, and then you should save the solved object, which contains all the information necessary to execute the dependency (dependency being in this case the user's endpoint/controller function). This is very important for performance: we want to do the least amount of work possible for each incoming request. from typing import Any , Callable from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value class App : def __init__ ( self , controller : Callable [ ... , Any ]) -> None : self . container = Container () self . solved = self . container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ], ) self . executor = SyncExecutor () def run ( self , request : Request ) -> int : with self . container . enter_scope ( \"request\" ) as state : return self . container . execute_sync ( self . solved , values = { Request : request }, executor = self . executor , state = state , ) # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) def main () -> None : app = App ( controller ) resp = app . run ( Request ( 1 )) assert resp == 2 resp = app . run ( Request ( 2 )) assert resp == 3 if __name__ == \"__main__\" : main () Finally, we execute the endpoint for each incoming request: from typing import Any , Callable from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value class App : def __init__ ( self , controller : Callable [ ... , Any ]) -> None : self . container = Container () self . solved = self . container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ], ) self . executor = SyncExecutor () def run ( self , request : Request ) -> int : with self . container . enter_scope ( \"request\" ) as state : return self . container . execute_sync ( self . solved , values = { Request : request }, executor = self . executor , state = state , ) # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) def main () -> None : app = App ( controller ) resp = app . run ( Request ( 1 )) assert resp == 2 resp = app . run ( Request ( 2 )) assert resp == 3 if __name__ == \"__main__\" : main () When we do this, we provide the Request instance as a value. This means that di does not introspect at all into the Request to figure out how to build it, it just hands the value off to anything that requests it. You can also directly register providers, which is covered in the provider registration section of the docs. You'll also notice the executor parameter. As you'll see in the [architecture] chapter, one of the fundamental design principles in di is to decouple wiring, solving and execution. This makes it trivial to, for example, enable concurrent execution of dependencies using threads, asynchronous task groups or any other execution paradigm you want.","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#simple-example","text":"Here is a simple example of how di works: from dataclasses import dataclass from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor class A : ... class B : ... @dataclass class C : a : A b : B def main (): container = Container () solved = container . solve ( Dependant ( C , scope = \"request\" ), scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : c = container . execute_sync ( solved , executor = SyncExecutor (), state = state ) assert isinstance ( c , C ) assert isinstance ( c . a , A ) assert isinstance ( c . b , B ) You will notice that di \"auto-wired\" C : we didn't have to tell it that C depends on A and B , or how to construct A and B , it was all inferred from type annotations. In the wiring and provider registration chapters, you'll see how you can customize this behavior to tell di how to inject things like abstract interfaces or function return values.","title":"Simple Example"},{"location":"examples/#in-depth-example","text":"With this background in place, let's dive into a more in-depth example. In this example, we'll look at what it would take for a web framework to provide dependency injection to its users via di . Let's start by looking at the User's code. from typing import Any , Callable from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value class App : def __init__ ( self , controller : Callable [ ... , Any ]) -> None : self . container = Container () self . solved = self . container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ], ) self . executor = SyncExecutor () def run ( self , request : Request ) -> int : with self . container . enter_scope ( \"request\" ) as state : return self . container . execute_sync ( self . solved , values = { Request : request }, executor = self . executor , state = state , ) # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) def main () -> None : app = App ( controller ) resp = app . run ( Request ( 1 )) assert resp == 2 resp = app . run ( Request ( 2 )) assert resp == 3 if __name__ == \"__main__\" : main () As a user, you have very little boilerplate. In fact, there is not a single line of code here that is not transmitting information. Now let's look at the web framework side of things. This part can get a bit complex, but it's okay because it's written once, in a library. First, we'll need to create a Container instance. This would be tied to the App or Router instance of the web framework. from typing import Any , Callable from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value class App : def __init__ ( self , controller : Callable [ ... , Any ]) -> None : self . container = Container () self . solved = self . container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ], ) self . executor = SyncExecutor () def run ( self , request : Request ) -> int : with self . container . enter_scope ( \"request\" ) as state : return self . container . execute_sync ( self . solved , values = { Request : request }, executor = self . executor , state = state , ) # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) def main () -> None : app = App ( controller ) resp = app . run ( Request ( 1 )) assert resp == 2 resp = app . run ( Request ( 2 )) assert resp == 3 if __name__ == \"__main__\" : main () Next we solve all of our endpoints/controllers (in this case just a single one). This should happen once, maybe at application startup, and then you should save the solved object, which contains all the information necessary to execute the dependency (dependency being in this case the user's endpoint/controller function). This is very important for performance: we want to do the least amount of work possible for each incoming request. from typing import Any , Callable from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value class App : def __init__ ( self , controller : Callable [ ... , Any ]) -> None : self . container = Container () self . solved = self . container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ], ) self . executor = SyncExecutor () def run ( self , request : Request ) -> int : with self . container . enter_scope ( \"request\" ) as state : return self . container . execute_sync ( self . solved , values = { Request : request }, executor = self . executor , state = state , ) # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) def main () -> None : app = App ( controller ) resp = app . run ( Request ( 1 )) assert resp == 2 resp = app . run ( Request ( 2 )) assert resp == 3 if __name__ == \"__main__\" : main () Finally, we execute the endpoint for each incoming request: from typing import Any , Callable from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value class App : def __init__ ( self , controller : Callable [ ... , Any ]) -> None : self . container = Container () self . solved = self . container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ], ) self . executor = SyncExecutor () def run ( self , request : Request ) -> int : with self . container . enter_scope ( \"request\" ) as state : return self . container . execute_sync ( self . solved , values = { Request : request }, executor = self . executor , state = state , ) # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) def main () -> None : app = App ( controller ) resp = app . run ( Request ( 1 )) assert resp == 2 resp = app . run ( Request ( 2 )) assert resp == 3 if __name__ == \"__main__\" : main () When we do this, we provide the Request instance as a value. This means that di does not introspect at all into the Request to figure out how to build it, it just hands the value off to anything that requests it. You can also directly register providers, which is covered in the provider registration section of the docs. You'll also notice the executor parameter. As you'll see in the [architecture] chapter, one of the fundamental design principles in di is to decouple wiring, solving and execution. This makes it trivial to, for example, enable concurrent execution of dependencies using threads, asynchronous task groups or any other execution paradigm you want.","title":"In-depth example"},{"location":"scopes/","text":"Scopes Scopes are one of the fundamental concepts in dependency injection. Some dependency injection frameworks provide fixed scopes, for example: Singleton: only one instance is created Request: in web frameworks, this could be the lifetime of a request Prototype: re-initialized every time it is needed di generalizes this concept by putting control of scopes into the hands of the users / implementers: a scope in di is identified by any hashable value (a string, enum, int, etc.) and entering / exiting scopes is handled via context managers: async with container . enter_scope ( \"app\" ): async with container . enter_scope ( \"request\" ): async with container . enter_scope ( \"foo, bar, baz!\" ): Scopes provide a framework for several other important features: Dependency lifespans Dependency value sharing Every dependency is linked to a scope. When a scope exits, all dependencies linked to it are destroyed (if they have teardown, the teardown is run) and their value is removed from the cache. This means that dependencies scoped to an outer scope cannot depend on dependencies scoped to an inner scope: from di.container import Container from di.dependant import Dependant , Marker from di.typing import Annotated class Request : ... RequestDep = Annotated [ Request , Marker ( scope = \"request\" )] class DBConnection : def __init__ ( self , request : RequestDep ) -> None : ... DBConnDep = Annotated [ DBConnection , Marker ( scope = \"app\" )] def controller ( conn : DBConnDep ) -> None : ... def framework () -> None : container = Container () container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"app\" , \"request\" ]) This example will fail with di.exceptions.ScopeViolationError because an DBConnection is scoped to \"app\" so it cannot depend on Request which is scoped to \"request\" . The order of the scopes is determined by the scopes parameter to Container.solve . If you've used Pytest fixtures before, you're already familiar with these rules. In Pytest, a \"session\" scoped fixture cannot depend on a \"function\" scoped fixture. Overriding scopes You may encounter situations where you don't want to make your users explicitly set the scope for each dependency. For example, Spring defaults dependencies to the \"singleton\" scope. Our approach is to give you a callback that gets information on the current context (the scopes passed to Container.solve , the current DependantBase and the scopes of all of it's sub-dependencies) where you can inject your own logic for determining the right scope. Some examples of this include: A fixed default scope. You ignore all of the inputs and return a fixed value. This allows you to emulate Spring's behavior by returning a \"singleton\" scope or FastAPI's behavior by returning a \"connection\"/\"request\" scope. Try to assign the outermost valid scope. If the dependency depends on a \"request\" sub-dependency, you can't assign a \"singleton\" scope, so you assign the \"request\" scope. If there are no sub-dependencies or they all have the \"singleton\" scope, then you can assign the \"singleton\" scope. Here is an example of the simpler fixed-default behavior: import os from typing import Any , Sequence from di.api.dependencies import DependantBase from di.api.scopes import Scope from di.container import Container from di.dependant import Dependant , Marker from di.executors import AsyncExecutor from di.typing import Annotated # Framework code class Request : def __init__ ( self , domain : str ) -> None : self . domain = domain async def web_framework () -> None : container = Container () container . bind ( lambda param , dependant : Dependant ( Request , scope = \"request\" , wire = False ) if dependant . call is Request else None ) def scope_resolver ( dep : DependantBase [ Any ], subdep_scopes : Sequence [ Scope ], scopes : Sequence [ Scope ], ) -> Scope : if dep . scope is None : return \"request\" return dep . scope solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"singleton\" , \"request\" ], scope_resolver = scope_resolver , ) async with container . enter_scope ( \"singleton\" ) as singleton_state : os . environ [ \"domain\" ] = \"bar.example.com\" async with container . enter_scope ( \"request\" , state = singleton_state ) as request_state : status = await container . execute_async ( solved , values = { Request : Request ( \"bar.example.com\" )}, executor = AsyncExecutor (), state = request_state , ) assert status == 200 , status os . environ [ \"domain\" ] = \"foo.example.com\" async with container . enter_scope ( \"request\" , state = singleton_state ) as request_state : status = await container . execute_async ( solved , values = { Request : Request ( \"foo.example.com\" )}, executor = AsyncExecutor (), state = request_state , ) assert status == 200 , status # get_domain_from_env gets the \"request\" scope def get_domain_from_env () -> str : return os . environ [ \"domain\" ] # authorize gets the \"request\" scope def authorize ( request : Request , domain : Annotated [ str , Marker ( get_domain_from_env )], ) -> bool : return request . domain == domain async def controller ( authorized : Annotated [ bool , Marker ( authorize )]) -> int : if authorized : return 200 return 403 In this example we didn't provide a scope for get_domain_from_env , but di can see that it does not depend on anything with the \"request\" scope and so it gets assigned the \"singleton\" scope. On the other hand authorize does depend on a Request object, so it gets the \"request\" scope.","title":"Scopes"},{"location":"scopes/#scopes","text":"Scopes are one of the fundamental concepts in dependency injection. Some dependency injection frameworks provide fixed scopes, for example: Singleton: only one instance is created Request: in web frameworks, this could be the lifetime of a request Prototype: re-initialized every time it is needed di generalizes this concept by putting control of scopes into the hands of the users / implementers: a scope in di is identified by any hashable value (a string, enum, int, etc.) and entering / exiting scopes is handled via context managers: async with container . enter_scope ( \"app\" ): async with container . enter_scope ( \"request\" ): async with container . enter_scope ( \"foo, bar, baz!\" ): Scopes provide a framework for several other important features: Dependency lifespans Dependency value sharing Every dependency is linked to a scope. When a scope exits, all dependencies linked to it are destroyed (if they have teardown, the teardown is run) and their value is removed from the cache. This means that dependencies scoped to an outer scope cannot depend on dependencies scoped to an inner scope: from di.container import Container from di.dependant import Dependant , Marker from di.typing import Annotated class Request : ... RequestDep = Annotated [ Request , Marker ( scope = \"request\" )] class DBConnection : def __init__ ( self , request : RequestDep ) -> None : ... DBConnDep = Annotated [ DBConnection , Marker ( scope = \"app\" )] def controller ( conn : DBConnDep ) -> None : ... def framework () -> None : container = Container () container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"app\" , \"request\" ]) This example will fail with di.exceptions.ScopeViolationError because an DBConnection is scoped to \"app\" so it cannot depend on Request which is scoped to \"request\" . The order of the scopes is determined by the scopes parameter to Container.solve . If you've used Pytest fixtures before, you're already familiar with these rules. In Pytest, a \"session\" scoped fixture cannot depend on a \"function\" scoped fixture.","title":"Scopes"},{"location":"scopes/#overriding-scopes","text":"You may encounter situations where you don't want to make your users explicitly set the scope for each dependency. For example, Spring defaults dependencies to the \"singleton\" scope. Our approach is to give you a callback that gets information on the current context (the scopes passed to Container.solve , the current DependantBase and the scopes of all of it's sub-dependencies) where you can inject your own logic for determining the right scope. Some examples of this include: A fixed default scope. You ignore all of the inputs and return a fixed value. This allows you to emulate Spring's behavior by returning a \"singleton\" scope or FastAPI's behavior by returning a \"connection\"/\"request\" scope. Try to assign the outermost valid scope. If the dependency depends on a \"request\" sub-dependency, you can't assign a \"singleton\" scope, so you assign the \"request\" scope. If there are no sub-dependencies or they all have the \"singleton\" scope, then you can assign the \"singleton\" scope. Here is an example of the simpler fixed-default behavior: import os from typing import Any , Sequence from di.api.dependencies import DependantBase from di.api.scopes import Scope from di.container import Container from di.dependant import Dependant , Marker from di.executors import AsyncExecutor from di.typing import Annotated # Framework code class Request : def __init__ ( self , domain : str ) -> None : self . domain = domain async def web_framework () -> None : container = Container () container . bind ( lambda param , dependant : Dependant ( Request , scope = \"request\" , wire = False ) if dependant . call is Request else None ) def scope_resolver ( dep : DependantBase [ Any ], subdep_scopes : Sequence [ Scope ], scopes : Sequence [ Scope ], ) -> Scope : if dep . scope is None : return \"request\" return dep . scope solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"singleton\" , \"request\" ], scope_resolver = scope_resolver , ) async with container . enter_scope ( \"singleton\" ) as singleton_state : os . environ [ \"domain\" ] = \"bar.example.com\" async with container . enter_scope ( \"request\" , state = singleton_state ) as request_state : status = await container . execute_async ( solved , values = { Request : Request ( \"bar.example.com\" )}, executor = AsyncExecutor (), state = request_state , ) assert status == 200 , status os . environ [ \"domain\" ] = \"foo.example.com\" async with container . enter_scope ( \"request\" , state = singleton_state ) as request_state : status = await container . execute_async ( solved , values = { Request : Request ( \"foo.example.com\" )}, executor = AsyncExecutor (), state = request_state , ) assert status == 200 , status # get_domain_from_env gets the \"request\" scope def get_domain_from_env () -> str : return os . environ [ \"domain\" ] # authorize gets the \"request\" scope def authorize ( request : Request , domain : Annotated [ str , Marker ( get_domain_from_env )], ) -> bool : return request . domain == domain async def controller ( authorized : Annotated [ bool , Marker ( authorize )]) -> int : if authorized : return 200 return 403 In this example we didn't provide a scope for get_domain_from_env , but di can see that it does not depend on anything with the \"request\" scope and so it gets assigned the \"singleton\" scope. On the other hand authorize does depend on a Request object, so it gets the \"request\" scope.","title":"Overriding scopes"},{"location":"solving/","text":"Solving Solving a dependency means build a directed acyclic graph (DAG) of dependencies by inspecting sub dependencies and resolving binds. Once we solve a dependency, we can execute it without doing any introspection. Solving is done by the Container . The result of solving is stored in a SolvedDependant object which you can pass to Container.execute_{sync,async} to get back the result. The simplest form of executing a dependency is thus: result = container . execute ( container . solve ( Dependant ( lambda : 1 ))) For a more comprehensive overview, see the architecture section. SolvedDependant di lets you pre-solve your dependencies so that you don't have to run the solver each time you execute. This usually comes with a huge performance boost, but only works if you have a static dependency graph. In practice, this just means that solving captures the current binds and won't be updated if there are changes to binds. Note that you can still have values in your DAG change, just not the shape of the DAG itself. For example, here is a more advanced use case where the framework solves the endpoint and then provides the Request as a value each time the endpoint is called. This means that di does not do any reflection for each request, nor does it have to do dependency resolution. from di.api.solved import SolvedDependant from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : ... def web_framework (): container = Container () solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ]) assert isinstance ( solved , SolvedDependant ) with container . enter_scope ( \"request\" ) as state : container . execute_sync ( solved , values = { Request : Request ()}, executor = SyncExecutor (), state = state ) dependencies = solved . dag . keys () - { solved . dependency } assert all ( isinstance ( item , Dependant ) for item in dependencies ) assert set ( dependant . call for dependant in dependencies ) == { Request , MyClass } # User code class MyClass : ... def controller ( request : Request , myobj : MyClass ) -> None : ... Getting a list of dependencies You can easily list all dependencies in a dag via SolvedDependant.dag.keys() . from di.api.solved import SolvedDependant from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : ... def web_framework (): container = Container () solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ]) assert isinstance ( solved , SolvedDependant ) with container . enter_scope ( \"request\" ) as state : container . execute_sync ( solved , values = { Request : Request ()}, executor = SyncExecutor (), state = state ) dependencies = solved . dag . keys () - { solved . dependency } assert all ( isinstance ( item , Dependant ) for item in dependencies ) assert set ( dependant . call for dependant in dependencies ) == { Request , MyClass } # User code class MyClass : ... def controller ( request : Request , myobj : MyClass ) -> None : ... This lists all of the Dependants for the solved dependency. This means that you can create custom markers and easily enumerate them. For example, you might make a Header dependency and then want to know what headers are being requested by the controller, even if they are nested inside other dependencies: from di import Dependant class Header ( Dependant [ str ]): ... See the dependants section for a more complete example of this.","title":"Solving"},{"location":"solving/#solving","text":"Solving a dependency means build a directed acyclic graph (DAG) of dependencies by inspecting sub dependencies and resolving binds. Once we solve a dependency, we can execute it without doing any introspection. Solving is done by the Container . The result of solving is stored in a SolvedDependant object which you can pass to Container.execute_{sync,async} to get back the result. The simplest form of executing a dependency is thus: result = container . execute ( container . solve ( Dependant ( lambda : 1 ))) For a more comprehensive overview, see the architecture section.","title":"Solving"},{"location":"solving/#solveddependant","text":"di lets you pre-solve your dependencies so that you don't have to run the solver each time you execute. This usually comes with a huge performance boost, but only works if you have a static dependency graph. In practice, this just means that solving captures the current binds and won't be updated if there are changes to binds. Note that you can still have values in your DAG change, just not the shape of the DAG itself. For example, here is a more advanced use case where the framework solves the endpoint and then provides the Request as a value each time the endpoint is called. This means that di does not do any reflection for each request, nor does it have to do dependency resolution. from di.api.solved import SolvedDependant from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : ... def web_framework (): container = Container () solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ]) assert isinstance ( solved , SolvedDependant ) with container . enter_scope ( \"request\" ) as state : container . execute_sync ( solved , values = { Request : Request ()}, executor = SyncExecutor (), state = state ) dependencies = solved . dag . keys () - { solved . dependency } assert all ( isinstance ( item , Dependant ) for item in dependencies ) assert set ( dependant . call for dependant in dependencies ) == { Request , MyClass } # User code class MyClass : ... def controller ( request : Request , myobj : MyClass ) -> None : ...","title":"SolvedDependant"},{"location":"solving/#getting-a-list-of-dependencies","text":"You can easily list all dependencies in a dag via SolvedDependant.dag.keys() . from di.api.solved import SolvedDependant from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor # Framework code class Request : ... def web_framework (): container = Container () solved = container . solve ( Dependant ( controller , scope = \"request\" ), scopes = [ \"request\" ]) assert isinstance ( solved , SolvedDependant ) with container . enter_scope ( \"request\" ) as state : container . execute_sync ( solved , values = { Request : Request ()}, executor = SyncExecutor (), state = state ) dependencies = solved . dag . keys () - { solved . dependency } assert all ( isinstance ( item , Dependant ) for item in dependencies ) assert set ( dependant . call for dependant in dependencies ) == { Request , MyClass } # User code class MyClass : ... def controller ( request : Request , myobj : MyClass ) -> None : ... This lists all of the Dependants for the solved dependency. This means that you can create custom markers and easily enumerate them. For example, you might make a Header dependency and then want to know what headers are being requested by the controller, even if they are nested inside other dependencies: from di import Dependant class Header ( Dependant [ str ]): ... See the dependants section for a more complete example of this.","title":"Getting a list of dependencies"},{"location":"wiring/","text":"Wiring Wiring is the act of \"connecting\" together dependencies. In di , wiring is handled by the Dependant API. The general idea is that Container accepts a Dependant and then asks it for it's sub-dependencies. These sub-dependencies are themselves Dependant s, and so the Container keeps asking them for their sub-dependenices until there are none. But how does Dependant know what it's dependencies are? Every Dependant has a call attribute which is a callable (a class, a function, etc.) that which can be introspected (usually with inpsect.signature ) to find it's parameters. The from these parameters the Dependant determine's what it's dependencies are. But how do we go from a parameter param: Foo to a Dependant ? There are actually several different mechanisms available: Autowiring Autowiring is available when the parameter's type annotation is a well-behaved type/class. Well behaved in this case means just means that it's parameters can be understood by di , for example that they are type annotated and are uniquely identifiable ( param: int won't work properly). Here is an example showing auto-wiring in action. Auto-wiring can work with dataclasses, even ones with a default_factory . In this example we'll load a config from the environment: from dataclasses import dataclass from di.container import Container from di.dependant import Dependant from di.executors import AsyncExecutor @dataclass class Config : host : str = \"localhost\" class DBConn : def __init__ ( self , config : Config ) -> None : self . host = config . host async def endpoint ( conn : DBConn ) -> None : assert isinstance ( conn , DBConn ) async def framework (): container = Container () solved = container . solve ( Dependant ( endpoint , scope = \"request\" ), scopes = [ \"request\" ]) async with container . enter_scope ( \"request\" ) as state : await container . execute_async ( solved , executor = AsyncExecutor (), state = state ) What makes this \"auto-wiring\" is that we didn't have to tell di how to construct DBConn : di detected that controller needed a DBConn and that DBConn in turn needs a Config instance. This is the simplest option because you don't have to do anything, but it' relatively limited in terms of what can be injected. Autowiring metadata To execute a dependency, di needs both a callable target (a class, function, etc.) and some metadata, namely scope and use_cache . Autowiring can discover the callable target from type annotations, but it cannot infer the metadata. So metadata is just inherited from the parent dependency: in the example above, we declared endpoint as having a \"request\" scope, so all of the sub-dependencies that get auto-wired end up having the \"request\" scope. Dependency markers Dependency markers, in the form of di.dependant.Marker serve to hold information about a dependency, for example how to construct it or it's scope. Markers are generally useful when: Injecting a non-identifiabletype, like a list[str] Injecting the result of a function ( param: some_function is not valid in Python) The type being injected is not well-behaved and you need to tell di how to construct it You want to attach metadata to the target (like explicitly setting the scope ) Let's take our previous example and look at how we would have used markers if DBConn accepted a host: str paramter instead of our Config class directly: from dataclasses import dataclass from di.container import Container from di.dependant import Dependant , Marker from di.executors import SyncExecutor from di.typing import Annotated @dataclass class Config : host : str = \"localhost\" class DBConn : def __init__ ( self , host : str ) -> None : self . host = host def inject_db ( config : Config ) -> DBConn : return DBConn ( host = config . host ) def endpoint ( conn : Annotated [ DBConn , Marker ( inject_db , scope = \"request\" )]) -> None : assert isinstance ( conn , DBConn ) def framework (): container = Container () solved = container . solve ( Dependant ( endpoint , scope = \"request\" ), scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : container . execute_sync ( solved , executor = SyncExecutor (), state = state ) All we had to do was tell di how to construct DBConn (by assigning the parameter a Marker ) and di can do the rest. Note that we are still using autowiring for endpoint and Config , it's not all or nothing and you can mix and match styles. A note on Annotated / PEP 593 Markers are set via PEP 593's Annotated . This is in contrast to FastAPIs use of markers as default values ( param: int = Depends(...) ). When FastAPI was designed, PEP 593 did not exist, and there are several advantages to using PEP 593's Annotated: Compatible with other uses of default values, like dataclass' field or Pydantic's Field . Non-invasive modification of signatures: adding Marker(...) in Annotated should be ignored by anything except di . Functions/classes can be called as normal outside of di and the default values (when present) will be used. Multiple markers can be used. For example, something like Annotated[T, PydanticField(), Marker()] . This last point is important because of the composability it provides: from typing import TypeVar , Annotated from di import Marker from pydantic import Field T_int = TypeVar ( \"T_int\" , bound = int ) PositiveInt = Annotated [ T_int , Field ( ge = 0 )] T = TypeVar ( \"T\" ) Depends = Annotated [ T , Marker ()] def foo ( v : Depends [ PositiveInt [ int ]]) -> int : return v Note how we used type aliases to create stackable, reusable types. This means that while Annotated can sometimes be verbose, it can also be made very convenient with type aliases. Custom types If you are writing and injecting your own classes, you also have the option of putting the dependency injection metadata into the class itself, via the __di_dependency__(cls) -> Marker protocol. This obviously doesn't work if you are injecting a 3rd party class you are importing (unless you subclass it). The main advantage of this method is that the consumers of this class (wich may be your own codebase) don't have to apply markers everywhere or worry about inconsistent scopes (see scopes ). For example, we can tell di constructing a class asynchronously`: import inspect from dataclasses import dataclass from di.container import Container from di.dependant import Dependant from di.executors import AsyncExecutor class HTTPClient : pass @dataclass class B : msg : str @classmethod def __di_dependency__ ( cls , param : inspect . Parameter ) -> \"Dependant[B]\" : # note that client is injected by di! async def func ( client : HTTPClient ) -> B : # do an http rquest or something return B ( msg = f \"\ud83d\udc4b from { param . name } \" ) return Dependant ( func ) async def main () -> None : def endpoint ( b : B ) -> str : return b . msg container = Container () executor = AsyncExecutor () solved = container . solve ( Dependant ( endpoint ), scopes = ( None ,)) async with container . enter_scope ( None ) as state : res = await container . execute_async ( solved , executor = executor , state = state ) assert res == \"\ud83d\udc4b from b\" This allows you to construct your class even if it depends on doing async work and it needs to refer to the class itself. If you only need to do async work and don't need access to the class, you don't need to use this and can instead just make your field depend on an asynchronous function: from dataclasses import dataclass from di.container import Container from di.dependant import Dependant , Marker from di.executors import AsyncExecutor from di.typing import Annotated async def get_msg () -> str : # make an http request or something return \"\ud83d\udc4b\" @dataclass class B : msg : Annotated [ str , Marker ( get_msg )] async def main () -> None : def endpoint ( b : B ) -> str : return b . msg container = Container () executor = AsyncExecutor () solved = container . solve ( Dependant ( endpoint ), scopes = ( None ,)) async with container . enter_scope ( None ) as state : res = await container . execute_async ( solved , executor = executor , state = state ) assert res == \"\ud83d\udc4b\" Another way this is useful is to pre-declare scopes for a class. For example, you may only want to have one UserRepo for you entire app: import inspect from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor class UsersRepo : @classmethod def __di_dependency__ ( cls , param : inspect . Parameter ) -> \"Dependant[UsersRepo]\" : return Dependant ( UsersRepo , scope = \"app\" ) def endpoint ( repo : UsersRepo ) -> UsersRepo : return repo def framework (): container = Container () solved = container . solve ( Dependant ( endpoint , scope = \"request\" ), scopes = [ \"app\" , \"request\" ] ) executor = SyncExecutor () with container . enter_scope ( \"app\" ) as app_state : with container . enter_scope ( \"request\" , state = app_state ) as req_state : repo1 = container . execute_sync ( solved , executor = executor , state = req_state ) with container . enter_scope ( \"request\" , state = app_state ) as req_state : repo2 = container . execute_sync ( solved , executor = executor , state = req_state ) assert repo1 is repo2 InjectableClass As a convenience, di provides an InjectableClass type that you can inherit from so that you can easily pass parameters to Marker without implementing __di_dependant__ : from di.container import Container from di.dependant import Dependant , Injectable from di.executors import SyncExecutor class UsersRepo ( Injectable , scope = \"app\" ): pass def endpoint ( repo : UsersRepo ) -> UsersRepo : return repo def framework (): container = Container () solved = container . solve ( Dependant ( endpoint , scope = \"request\" ), scopes = [ \"app\" , \"request\" ] ) executor = SyncExecutor () with container . enter_scope ( \"app\" ) as app_state : with container . enter_scope ( \"request\" , state = app_state ) as request_state : repo1 = container . execute_sync ( solved , executor = executor , state = request_state ) with container . enter_scope ( \"request\" ): repo2 = container . execute_sync ( solved , executor = executor , state = request_state ) assert repo1 is repo2 Binds Binds, which will be covered in depth in the binds section offer a way of swapping out dependencies imperatively (when you encounter type \"X\", use function \"y\" to build it). They can be used with any of the methods described above. Performance Reflection (inspecting function signatures for dependencies) is very slow . For this reason, di tries to avoid it as much as possible. The best way to avoid extra introspection is to re-use Solved Dependants . Conclusion There are several ways to declare dependencies in di . Which one makes sense for each use case depends on several factors, but ultimately they all achieve the same outcome.","title":"Wiring"},{"location":"wiring/#wiring","text":"Wiring is the act of \"connecting\" together dependencies. In di , wiring is handled by the Dependant API. The general idea is that Container accepts a Dependant and then asks it for it's sub-dependencies. These sub-dependencies are themselves Dependant s, and so the Container keeps asking them for their sub-dependenices until there are none. But how does Dependant know what it's dependencies are? Every Dependant has a call attribute which is a callable (a class, a function, etc.) that which can be introspected (usually with inpsect.signature ) to find it's parameters. The from these parameters the Dependant determine's what it's dependencies are. But how do we go from a parameter param: Foo to a Dependant ? There are actually several different mechanisms available:","title":"Wiring"},{"location":"wiring/#autowiring","text":"Autowiring is available when the parameter's type annotation is a well-behaved type/class. Well behaved in this case means just means that it's parameters can be understood by di , for example that they are type annotated and are uniquely identifiable ( param: int won't work properly). Here is an example showing auto-wiring in action. Auto-wiring can work with dataclasses, even ones with a default_factory . In this example we'll load a config from the environment: from dataclasses import dataclass from di.container import Container from di.dependant import Dependant from di.executors import AsyncExecutor @dataclass class Config : host : str = \"localhost\" class DBConn : def __init__ ( self , config : Config ) -> None : self . host = config . host async def endpoint ( conn : DBConn ) -> None : assert isinstance ( conn , DBConn ) async def framework (): container = Container () solved = container . solve ( Dependant ( endpoint , scope = \"request\" ), scopes = [ \"request\" ]) async with container . enter_scope ( \"request\" ) as state : await container . execute_async ( solved , executor = AsyncExecutor (), state = state ) What makes this \"auto-wiring\" is that we didn't have to tell di how to construct DBConn : di detected that controller needed a DBConn and that DBConn in turn needs a Config instance. This is the simplest option because you don't have to do anything, but it' relatively limited in terms of what can be injected.","title":"Autowiring"},{"location":"wiring/#autowiring-metadata","text":"To execute a dependency, di needs both a callable target (a class, function, etc.) and some metadata, namely scope and use_cache . Autowiring can discover the callable target from type annotations, but it cannot infer the metadata. So metadata is just inherited from the parent dependency: in the example above, we declared endpoint as having a \"request\" scope, so all of the sub-dependencies that get auto-wired end up having the \"request\" scope.","title":"Autowiring metadata"},{"location":"wiring/#dependency-markers","text":"Dependency markers, in the form of di.dependant.Marker serve to hold information about a dependency, for example how to construct it or it's scope. Markers are generally useful when: Injecting a non-identifiabletype, like a list[str] Injecting the result of a function ( param: some_function is not valid in Python) The type being injected is not well-behaved and you need to tell di how to construct it You want to attach metadata to the target (like explicitly setting the scope ) Let's take our previous example and look at how we would have used markers if DBConn accepted a host: str paramter instead of our Config class directly: from dataclasses import dataclass from di.container import Container from di.dependant import Dependant , Marker from di.executors import SyncExecutor from di.typing import Annotated @dataclass class Config : host : str = \"localhost\" class DBConn : def __init__ ( self , host : str ) -> None : self . host = host def inject_db ( config : Config ) -> DBConn : return DBConn ( host = config . host ) def endpoint ( conn : Annotated [ DBConn , Marker ( inject_db , scope = \"request\" )]) -> None : assert isinstance ( conn , DBConn ) def framework (): container = Container () solved = container . solve ( Dependant ( endpoint , scope = \"request\" ), scopes = [ \"request\" ]) with container . enter_scope ( \"request\" ) as state : container . execute_sync ( solved , executor = SyncExecutor (), state = state ) All we had to do was tell di how to construct DBConn (by assigning the parameter a Marker ) and di can do the rest. Note that we are still using autowiring for endpoint and Config , it's not all or nothing and you can mix and match styles.","title":"Dependency markers"},{"location":"wiring/#a-note-on-annotated-pep-593","text":"Markers are set via PEP 593's Annotated . This is in contrast to FastAPIs use of markers as default values ( param: int = Depends(...) ). When FastAPI was designed, PEP 593 did not exist, and there are several advantages to using PEP 593's Annotated: Compatible with other uses of default values, like dataclass' field or Pydantic's Field . Non-invasive modification of signatures: adding Marker(...) in Annotated should be ignored by anything except di . Functions/classes can be called as normal outside of di and the default values (when present) will be used. Multiple markers can be used. For example, something like Annotated[T, PydanticField(), Marker()] . This last point is important because of the composability it provides: from typing import TypeVar , Annotated from di import Marker from pydantic import Field T_int = TypeVar ( \"T_int\" , bound = int ) PositiveInt = Annotated [ T_int , Field ( ge = 0 )] T = TypeVar ( \"T\" ) Depends = Annotated [ T , Marker ()] def foo ( v : Depends [ PositiveInt [ int ]]) -> int : return v Note how we used type aliases to create stackable, reusable types. This means that while Annotated can sometimes be verbose, it can also be made very convenient with type aliases.","title":"A note on Annotated / PEP 593"},{"location":"wiring/#custom-types","text":"If you are writing and injecting your own classes, you also have the option of putting the dependency injection metadata into the class itself, via the __di_dependency__(cls) -> Marker protocol. This obviously doesn't work if you are injecting a 3rd party class you are importing (unless you subclass it). The main advantage of this method is that the consumers of this class (wich may be your own codebase) don't have to apply markers everywhere or worry about inconsistent scopes (see scopes ). For example, we can tell di constructing a class asynchronously`: import inspect from dataclasses import dataclass from di.container import Container from di.dependant import Dependant from di.executors import AsyncExecutor class HTTPClient : pass @dataclass class B : msg : str @classmethod def __di_dependency__ ( cls , param : inspect . Parameter ) -> \"Dependant[B]\" : # note that client is injected by di! async def func ( client : HTTPClient ) -> B : # do an http rquest or something return B ( msg = f \"\ud83d\udc4b from { param . name } \" ) return Dependant ( func ) async def main () -> None : def endpoint ( b : B ) -> str : return b . msg container = Container () executor = AsyncExecutor () solved = container . solve ( Dependant ( endpoint ), scopes = ( None ,)) async with container . enter_scope ( None ) as state : res = await container . execute_async ( solved , executor = executor , state = state ) assert res == \"\ud83d\udc4b from b\" This allows you to construct your class even if it depends on doing async work and it needs to refer to the class itself. If you only need to do async work and don't need access to the class, you don't need to use this and can instead just make your field depend on an asynchronous function: from dataclasses import dataclass from di.container import Container from di.dependant import Dependant , Marker from di.executors import AsyncExecutor from di.typing import Annotated async def get_msg () -> str : # make an http request or something return \"\ud83d\udc4b\" @dataclass class B : msg : Annotated [ str , Marker ( get_msg )] async def main () -> None : def endpoint ( b : B ) -> str : return b . msg container = Container () executor = AsyncExecutor () solved = container . solve ( Dependant ( endpoint ), scopes = ( None ,)) async with container . enter_scope ( None ) as state : res = await container . execute_async ( solved , executor = executor , state = state ) assert res == \"\ud83d\udc4b\" Another way this is useful is to pre-declare scopes for a class. For example, you may only want to have one UserRepo for you entire app: import inspect from di.container import Container from di.dependant import Dependant from di.executors import SyncExecutor class UsersRepo : @classmethod def __di_dependency__ ( cls , param : inspect . Parameter ) -> \"Dependant[UsersRepo]\" : return Dependant ( UsersRepo , scope = \"app\" ) def endpoint ( repo : UsersRepo ) -> UsersRepo : return repo def framework (): container = Container () solved = container . solve ( Dependant ( endpoint , scope = \"request\" ), scopes = [ \"app\" , \"request\" ] ) executor = SyncExecutor () with container . enter_scope ( \"app\" ) as app_state : with container . enter_scope ( \"request\" , state = app_state ) as req_state : repo1 = container . execute_sync ( solved , executor = executor , state = req_state ) with container . enter_scope ( \"request\" , state = app_state ) as req_state : repo2 = container . execute_sync ( solved , executor = executor , state = req_state ) assert repo1 is repo2","title":"Custom types"},{"location":"wiring/#injectableclass","text":"As a convenience, di provides an InjectableClass type that you can inherit from so that you can easily pass parameters to Marker without implementing __di_dependant__ : from di.container import Container from di.dependant import Dependant , Injectable from di.executors import SyncExecutor class UsersRepo ( Injectable , scope = \"app\" ): pass def endpoint ( repo : UsersRepo ) -> UsersRepo : return repo def framework (): container = Container () solved = container . solve ( Dependant ( endpoint , scope = \"request\" ), scopes = [ \"app\" , \"request\" ] ) executor = SyncExecutor () with container . enter_scope ( \"app\" ) as app_state : with container . enter_scope ( \"request\" , state = app_state ) as request_state : repo1 = container . execute_sync ( solved , executor = executor , state = request_state ) with container . enter_scope ( \"request\" ): repo2 = container . execute_sync ( solved , executor = executor , state = request_state ) assert repo1 is repo2","title":"InjectableClass"},{"location":"wiring/#binds","text":"Binds, which will be covered in depth in the binds section offer a way of swapping out dependencies imperatively (when you encounter type \"X\", use function \"y\" to build it). They can be used with any of the methods described above.","title":"Binds"},{"location":"wiring/#performance","text":"Reflection (inspecting function signatures for dependencies) is very slow . For this reason, di tries to avoid it as much as possible. The best way to avoid extra introspection is to re-use Solved Dependants .","title":"Performance"},{"location":"wiring/#conclusion","text":"There are several ways to declare dependencies in di . Which one makes sense for each use case depends on several factors, but ultimately they all achieve the same outcome.","title":"Conclusion"}]}